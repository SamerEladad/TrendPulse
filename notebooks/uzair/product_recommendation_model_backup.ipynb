{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, auc, accuracy_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR, MultiplicativeLR, ExponentialLR\n",
    "import torch.nn.functional as F\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Train and Test Data for complete isolation and avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\Data_Science\\Downloads\\trend_pulse_repo\\TrendPulse\\data\\clean\\Combined_Amazon_Dataset_Cleaned.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df = df.rename(columns={'name': 'product'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['main_category'], random_state=42)\n",
    "train_df.to_excel(r'C:\\Users\\Data_Science\\Desktop\\trend_pulse_data\\Train_data.xlsx',  index=False)\n",
    "test_df.to_excel(r'C:\\Users\\Data_Science\\Desktop\\trend_pulse_data\\Test_data.xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29639, 8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the separated Training Data for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the separated training data into df\n",
    "\n",
    "df = pd.read_excel(r'C:\\Users\\Data_Science\\Desktop\\trend_pulse_data\\Train_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create manual mappings for categorical columns\n",
    "# Mapping products\n",
    "unique_products = df['product'].unique()\n",
    "product_to_index = {product: idx for idx, product in enumerate(unique_products)}\n",
    "df['encoded_product'] = df['product'].map(product_to_index)\n",
    "\n",
    "# Mapping main categories\n",
    "unique_main_categories = df['main_category'].unique()\n",
    "main_category_to_index = {category: idx for idx, category in enumerate(unique_main_categories)}\n",
    "df['encoded_main_category'] = df['main_category'].map(main_category_to_index)\n",
    "\n",
    "# Mapping subcategories\n",
    "unique_sub_categories = df['sub_category'].unique()\n",
    "sub_category_to_index = {subcategory: idx for idx, subcategory in enumerate(unique_sub_categories)}\n",
    "df['encoded_sub_category'] = df['sub_category'].map(sub_category_to_index)\n",
    "\n",
    "# Create the int_rating column using a lambda function with three categories of ratings\n",
    "df['int_rating'] = df['ratings'].apply(lambda x: 1 if x <= 2.5 else (2 if x <= 4.0 else 3))\n",
    "\n",
    "\n",
    "# Convert prices from rupees to USD\n",
    "df['actual_price'] = df['actual_price'] / 83\n",
    "df['discount_price'] = df['discount_price'] / 83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: Process the 'link' column if necessary or drop it\n",
    "# df = train_df.drop(columns=['link'])\n",
    "\n",
    "# Step 2: Normalize numerical columns\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numerical features\n",
    "df[['ratings', 'no_of_ratings', 'actual_price', 'discount_price']] = scaler.fit_transform(df[['ratings', 'no_of_ratings', 'actual_price', 'discount_price']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = df[['encoded_product', 'encoded_main_category', 'encoded_sub_category', 'ratings', 'no_of_ratings', 'actual_price', 'discount_price']]\n",
    "y = df['int_rating']\n",
    "\n",
    "# Stratified split to maintain the distribution of int_rating across train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Combine features and target back into DataFrames\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "val_df = pd.concat([X_val, y_val], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert training data to PyTorch tensors\n",
    "train_product_ids = torch.tensor(train_df['encoded_product'].values, dtype=torch.long)\n",
    "train_category_ids = torch.tensor(train_df['encoded_main_category'].values, dtype=torch.long)\n",
    "train_subcategory_ids = torch.tensor(train_df['encoded_sub_category'].values, dtype=torch.long)\n",
    "train_ratings = torch.tensor(train_df['ratings'].values, dtype=torch.float32).view(-1, 1)\n",
    "train_no_of_ratings = torch.tensor(train_df['no_of_ratings'].values, dtype=torch.float32).view(-1, 1)\n",
    "train_actual_price = torch.tensor(train_df['actual_price'].values, dtype=torch.float32).view(-1, 1)\n",
    "train_discount_price = torch.tensor(train_df['discount_price'].values, dtype=torch.float32).view(-1, 1)\n",
    "train_labels = torch.tensor(train_df['int_rating'].values, dtype=torch.long)  # CrossEntropyLoss expects long dtype for labels\n",
    "\n",
    "# Convert validation data to PyTorch tensors\n",
    "val_product_ids = torch.tensor(val_df['encoded_product'].values, dtype=torch.long)\n",
    "val_category_ids = torch.tensor(val_df['encoded_main_category'].values, dtype=torch.long)\n",
    "val_subcategory_ids = torch.tensor(val_df['encoded_sub_category'].values, dtype=torch.long)\n",
    "val_ratings = torch.tensor(val_df['ratings'].values, dtype=torch.float32).view(-1, 1)\n",
    "val_no_of_ratings = torch.tensor(val_df['no_of_ratings'].values, dtype=torch.float32).view(-1, 1)\n",
    "val_actual_price = torch.tensor(val_df['actual_price'].values, dtype=torch.float32).view(-1, 1)\n",
    "val_discount_price = torch.tensor(val_df['discount_price'].values, dtype=torch.float32).view(-1, 1)\n",
    "val_labels = torch.tensor(val_df['int_rating'].values, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine tensors into TensorDatasets\n",
    "train_dataset = TensorDataset(train_product_ids, train_category_ids, train_subcategory_ids, train_ratings, train_no_of_ratings, train_actual_price, train_discount_price, train_labels)\n",
    "val_dataset = TensorDataset(val_product_ids, val_category_ids, val_subcategory_ids, val_ratings, val_no_of_ratings, val_actual_price, val_discount_price, val_labels)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# Check the first batch from the training DataLoader\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Convert train_data to tensors\n",
    "x_cat_train = torch.tensor(train_data[['main_category', 'sub_category']].values, dtype=torch.long)\n",
    "# Split the tensor into separate tensors for each category\n",
    "x_main_cat_train = x_cat_train[:, 0]  # Extract the first column\n",
    "x_sub_cat_train = x_cat_train[:, 1]   # Extract the second column\n",
    "\n",
    "# Combine them into a tuple\n",
    "x_cat_train_tuple = (x_main_cat_train, x_sub_cat_train)\n",
    "\n",
    "x_num_train = torch.tensor(train_data[['ratings', 'no_of_ratings', 'discount_price', 'actual_price']].values, dtype=torch.float32)\n",
    "y_train = torch.tensor(train_data['encoded_product'].values, dtype=torch.long)\n",
    "\n",
    "# Convert val_data to tensors\n",
    "x_cat_val = torch.tensor(val_data[['main_category', 'sub_category']].values, dtype=torch.long)\n",
    "# Split the tensor into separate tensors for each category\n",
    "x_main_cat_val = x_cat_val[:, 0]  # Extract the first column\n",
    "x_sub_cat_val = x_cat_val[:, 1]   # Extract the second column\n",
    "\n",
    "# Combine them into a tuple\n",
    "x_cat_val_tuple = (x_main_cat_val, x_sub_cat_val)\n",
    "x_num_val = torch.tensor(val_data[['ratings', 'no_of_ratings', 'discount_price', 'actual_price']].values, dtype=torch.float32)\n",
    "y_val = torch.tensor(val_data['encoded_product'].values, dtype=torch.long)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Dataset class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommendationModel(nn.Module):\n",
    "    def __init__(self, num_products, num_categories, num_subcategories, embedding_dim=10):\n",
    "        super(RecommendationModel, self).__init__()\n",
    "        \n",
    "        # Embedding layers for categorical features\n",
    "        self.product_embedding = nn.Embedding(num_products, embedding_dim)\n",
    "        self.category_embedding = nn.Embedding(num_categories, embedding_dim // 2)\n",
    "        self.subcategory_embedding = nn.Embedding(num_subcategories, embedding_dim // 2)\n",
    "        \n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(embedding_dim + (embedding_dim // 2) * 2 + 4, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 3)  # Output layer with 3 units for 3 classes\n",
    "\n",
    "    def forward(self, product_id, category_id, subcategory_id, ratings, no_of_ratings, actual_price, discount_price):\n",
    "        # Pass inputs through embeddings\n",
    "        product_embedded = self.product_embedding(product_id)\n",
    "        category_embedded = self.category_embedding(category_id)\n",
    "        subcategory_embedded = self.subcategory_embedding(subcategory_id)\n",
    "        \n",
    "        # Flatten the embedding outputs\n",
    "        product_embedded = product_embedded.view(-1, self.product_embedding.embedding_dim)\n",
    "        category_embedded = category_embedded.view(-1, self.category_embedding.embedding_dim)\n",
    "        subcategory_embedded = subcategory_embedded.view(-1, self.subcategory_embedding.embedding_dim)\n",
    "        \n",
    "        # Concatenate all features (embeddings + numerical features)\n",
    "        concatenated = torch.cat((product_embedded, category_embedded, subcategory_embedded, ratings, no_of_ratings, actual_price, discount_price), dim=1)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = torch.relu(self.fc1(concatenated))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        output = self.fc3(x)  # Output 3 units (for 3 classes) without activation\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics trackers\n",
    "def compute_accuracy(predictions, labels):\n",
    "    _, predicted = torch.max(predictions, 1)\n",
    "    return accuracy_score(labels.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "def compute_auc(predictions, labels, num_classes):\n",
    "    predictions_prob = F.softmax(predictions, dim=1).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    if num_classes == 2:\n",
    "        return roc_auc_score(labels, predictions_prob[:, 1])  # For binary classification\n",
    "    else:\n",
    "        return roc_auc_score(labels, predictions_prob, multi_class='ovr')  # For multi-class classification\n",
    "    \n",
    "def logging(log_path, log_text):\n",
    "    with open(log_path, 'a') as file:\n",
    "        file.write(log_text + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = r'C:\\Users\\Data_Science\\Desktop\\trend_pulse_data'\n",
    "experiment = 'Exp_1'\n",
    "exp_dir = os.path.join(main_dir, experiment)\n",
    "weights_dir = os.path.join(exp_dir, 'model_weights')\n",
    "log_path = os.path.join(exp_dir, 'Training_log.txt')\n",
    "os.makedirs(weights_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model initialization and Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "num_products = len(df['encoded_product'].unique())\n",
    "num_categories = len(df['encoded_main_category'].unique())\n",
    "num_subcategories = len(df['encoded_sub_category'].unique())\n",
    "\n",
    "model = RecommendationModel(num_products, num_categories, num_subcategories)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()  # Since it's a multi-class classification problem\n",
    "\n",
    "# Define Optimizer and Scheduler and Learning Rate\n",
    "\n",
    "num_epochs = 50\n",
    "initial_lr = 0.001\n",
    "exponent = 0.96\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_lr, weight_decay=3e-3)\n",
    "scheduler = ExponentialLR(optimizer, gamma=exponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecommendationModel(\n",
      "  (product_embedding): Embedding(28588, 10)\n",
      "  (category_embedding): Embedding(10, 5)\n",
      "  (subcategory_embedding): Embedding(12, 5)\n",
      "  (fc1): Linear(in_features=24, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch: 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[198], line 37\u001b[0m\n\u001b[0;32m     33\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear gradients\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(x_cat, x_num)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     40\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\regular_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\regular_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[192], line 21\u001b[0m, in \u001b[0;36mDNNModel.forward\u001b[1;34m(self, x_cat, x_num)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_cat, x_num):\n\u001b[1;32m---> 21\u001b[0m     x_main_cat, x_sub_cat \u001b[38;5;241m=\u001b[39m x_cat\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Apply embedding layers\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     x_main_cat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_main_category(x_main_cat)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize best validation loss\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Initialize lists to store predictions and labels\n",
    "train_preds_list = []\n",
    "train_labels_list = []\n",
    "val_preds_list = []\n",
    "val_labels_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch: {epoch + 1}/{num_epochs}')\n",
    "    logging(log_path, f'Epoch: {epoch + 1}/{num_epochs}')\n",
    "    \n",
    "    start_time = time.time()  # Start epoch timer\n",
    "    \n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    train_predictions = []\n",
    "    train_labels = []\n",
    "    \n",
    "    # Training loop\n",
    "    for (x_cat, x_num), labels in train_loader:\n",
    "        # Move data to the appropriate device\n",
    "        x_cat = x_cat.to(device)\n",
    "        x_num = x_num.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        \n",
    "        outputs = model(x_cat, x_num)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        \n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_predictions.append(outputs.cpu())\n",
    "        train_labels.append(labels.cpu())\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_predictions = torch.cat(train_predictions)\n",
    "    train_labels = torch.cat(train_labels)\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = compute_accuracy(train_predictions, train_labels)\n",
    "    train_auc = compute_auc(train_predictions, train_labels, num_products)\n",
    "    \n",
    "    # Save training predictions and labels\n",
    "    train_preds_list.append(train_predictions.numpy())\n",
    "    train_labels_list.append(train_labels.numpy())\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for (x_cat, x_num), labels in val_loader:\n",
    "            # Move data to the appropriate device\n",
    "            x_cat = x_cat.to(device)\n",
    "            x_num = x_num.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(x_cat, x_num)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            val_predictions.append(outputs.cpu())\n",
    "            val_labels.append(labels.cpu())\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_predictions = torch.cat(val_predictions)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = compute_accuracy(val_predictions, val_labels)\n",
    "    val_auc = compute_auc(val_predictions, val_labels, num_products)\n",
    "    \n",
    "    # Save validation predictions and labels\n",
    "    val_preds_list.append(val_predictions.numpy())\n",
    "    val_labels_list.append(val_labels.numpy())\n",
    "    \n",
    "    # Calculate epoch time\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    # Learning rate\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] | '\n",
    "          f'Train Loss: {train_loss:.4f} | '\n",
    "          f'Val Loss: {val_loss:.4f} | '\n",
    "          f'Train Accuracy: {train_accuracy:.4f} | '\n",
    "          f'Val Accuracy: {val_accuracy:.4f} | '\n",
    "          f'Train AUC: {train_auc:.4f} | '\n",
    "          f'Val AUC: {val_auc:.4f} | '\n",
    "          f'Epoch Time: {epoch_time:.2f}s | '\n",
    "          f'Learning Rate: {lr:.6f}')\n",
    "    \n",
    "    logging(log_path,\n",
    "            f'Epoch [{epoch+1}/{num_epochs}] | '\n",
    "          f'Train Loss: {train_loss:.4f} | '\n",
    "          f'Val Loss: {val_loss:.4f} | '\n",
    "          f'Train Accuracy: {train_accuracy:.4f} | '\n",
    "          f'Val Accuracy: {val_accuracy:.4f} | '\n",
    "          f'Train AUC: {train_auc:.4f} | '\n",
    "          f'Val AUC: {val_auc:.4f} | '\n",
    "          f'Epoch Time: {epoch_time:.2f}s | '\n",
    "          f'Learning Rate: {lr:.6f}')\n",
    "    \n",
    "    # Save the best model based on validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f'\\nSaved new best model with Train_AUC: {train_auc:.3f} --- Val_AUC: {val_auc:.3f}\\n')\n",
    "        logging(exp_dir, f'\\nSaved new best model with Train_AUC: {train_auc:.3f} --- Val_AUC: {val_auc:.3f}\\n')\n",
    "\n",
    "# After training, save all predictions and labels\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "train_preds_array = np.concatenate(train_preds_list, axis=0)\n",
    "train_labels_array = np.concatenate(train_labels_list, axis=0)\n",
    "val_preds_array = np.concatenate(val_preds_list, axis=0)\n",
    "val_labels_array = np.concatenate(val_labels_list, axis=0)\n",
    "\n",
    "# Create dataframes\n",
    "train_prob_df = pd.DataFrame({'predictions': train_preds_array.flatten(), 'labels': train_labels_array.flatten()})\n",
    "val_prob_df = pd.DataFrame({'predictions': val_preds_array.flatten(), 'labels': val_labels_array.flatten()})\n",
    "\n",
    "# Save dataframes to excel\n",
    "train_prob_df.to_excel(os.path.join(exp_dir, 'train_data.xlsx'), index=False)\n",
    "val_prob_df.to_excel(os.path.join(exp_dir,'val_data.xlsx'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regular_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
